<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- {{ ansible_managed }} -->

<configuration>

  <!-- common server name -->
  <property>
    <name>dfs.nameservices</name>
    <value>{{ site_name|lower }}</value>
  </property>

  <property>
    <name>hadoop.tmp.dir</name>
    <value>/disk1/tmp/hadoop-${user.name}</value>
    <final>true</final>
  </property>

  <!-- HA configuration, see http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/PDF/CDH4-High-Availability-Guide.pdf -->
  <property>
    <name>dfs.ha.namenodes.{{ site_name|lower }}</name>
    <value>{% for host in groups['namenodes'] %}nn{{ loop.index }}{% if not loop.last %},{% endif %}{% endfor %}</value>
  </property>

{% for host in groups['namenodes'] %}
  <property>
    <name>dfs.namenode.rpc-address.{{ site_name|lower }}.nn{{ loop.index }}</name>
    <value>{{ host }}:8020</value>
  </property>
{% endfor %}

{% for host in groups['namenodes'] %}
  <property>
    <name>dfs.namenode.http-address.{{ site_name|lower }}.nn{{ loop.index }}</name>
    <value>{{ host }}:50070</value>
  </property>
{% endfor %}

  <!-- Storage for edits' files -->
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://{% for host in groups['journalnodes'] %}{{ host }}:8485{% if not loop.last %};{% endif %}{% endfor %}/{{ site_name|lower }}</value>
  </property>

  <!-- Client failover -->
  <property>
    <name>dfs.client.failover.proxy.provider.{{ site_name|lower }}</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>

  <!-- Fencing configuration -->
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>shell(/bin/true)</value>
  </property>

  <!-- Automatic failover configuration -->
  <!-- <property> -->
  <!--   <name>dfs.ha.automatic-failover.enabled</name> -->
  <!--   <value>true</value> -->
  <!-- </property> -->
  <!-- <property> -->
  <!--   <name>ha.zookeeper.quorum</name> -->
  <!--   <value>{% for host in groups['zookeepers'] %}{{ host }}{% if not loop.last %},{% endif %}{% endfor %}</value> -->
  <!-- </property> -->

  <!-- Replication factor -->
  <property>
    <name>dfs.replication</name>
    <value>{{ dfs_replication }}</value>
  </property>

  <property>
    <name>dfs.blocksize</name>
    <value>{{ dfs_blocksize }}</value>
  </property>


  <!-- Directory Configuration -->
  <!-- This is the absolute path on the JournalNode machines where the edits and other -->
  <!-- local state (used by the JNs) will be stored. You may only use a single path for this -->
  <!-- configuration. Redundancy for this data is provided by either running multiple -->
  <!-- separate JournalNodes or by configuring this directory on a locally-attached RAID -->
  <!-- array -->
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>{{ dfs_journalnode_edits_dir }}</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>{% for dir in dfs_namenode_name_dir %}file://{{ dir }}{% if not loop.last %},{% endif %}{% endfor %}</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>{% for dir in dfs_datanode_data_dir %}file://{{ dir }}{% if not loop.last %},{% endif %}{% endfor %}</value>
  </property>

  <!-- The name of the group of super-users -->
  <property>
    <name>dfs.permissions.superusergroup</name>
    <value>{{ dfs_permissions_superusergroup }}</value>
  </property>

  <!-- turn off/on the permission checking -->
  <property>
    <name>dfs.permissions.enabled</name>
    <value>{{ dfs_permissions_enabled }}</value>
  </property>

  <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>{{ dfs_datanode_max_transfer_threads }}</value>
    <final>true</final>
  </property>

  <property>
    <name>dfs.namenode.handler.count</name>
    <value>{{ dfs_namenode_handler_count }}</value>
    <final>true</final>
  </property>

  <property>
    <name>dfs.datanode.handler.count</name>
    <value>{{ dfs_datanode_handler_count }}</value>
    <final>true</final>
  </property>

  <property>
    <name>dfs.namenode.avoid.read.stale.datanode</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.namenode.avoid.write.stale.datanode</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.namenode.stale.datanode.interval</name>
    <value>30000</value>
  </property>

  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.domain.socket.path</name>
    <value>/var/run/hadoop-hdfs/dn._PORT</value>
  </property>

  <property>
    <name>dfs.client.file-block-storage-locations.timeout</name>
    <value>3000</value>
  </property>

  <!-- <property> -->
  <!--   <name>dfs.hosts.exclude</name> -->
  <!--   <value>/etc/hadoop/conf.{{ site_name|lower }}/dfs.hosts.exclude</value> -->
  <!-- </property> -->

  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
  </property>

</configuration>
